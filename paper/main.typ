#import "@preview/charged-ieee:0.1.0": ieee

#show: ieee.with(
  title: [
    Legislative Narratives on Gun Control in the United States: A Sentiment Analysis Approach
  ],
  abstract: [
    This paper explores the emotions and sentiments of congressional gun legislation in the United States through sentiment analysis, using the VADER lexicon-based model to gain insights into the polarized landscape of the gun control debate in the U.S. Focusing on a dataset of 2030 legislative texts sourced from Congress.gov and spanning from 1989 to 2023, this study first classifies each piece of legislation as either pro-control or pro-rights and then conducts sentiment analysis to get legislative sentiment scores. The findings of the sentiment analysis show a pronounced split within the legislation, with minimal neutral sentiment and a tendency towards strong positive or negative sentiments. Statistical analyses using a Mann-Whitney U Test confirm significant differences in the central tendency of sentiments between pro-control and pro-rights legislation, with the pro-control legislation having more negative sentiment and pro-rights legislation displaying more positive sentiment. This research contributes to the current understanding of the legislative landscape of gun control, highlighting the stark polarization and potential implications for policy-making and public perception.
    #footnote[Code: https://github.com/ojas-chaturvedi/NLP-Gun-Legislation]
  ],
  authors: ((
    name: "Ojas Chaturvedi",
    department: [Student],
    organization: [BASIS Chandler High School],
    location: [Chandler, Arizona],
    email: "oj.chaturvedi.2024@gmail.com"
  ),),
  index-terms: (
    "Natural Language Processing",
    "Sentiment Analysis",
    "Gun Control",
    "VADER",
    "Congressional Legislation",
    "Large Language Models",
    "Machine Learning",
    "Politics",
  ),
  bibliography: bibliography("refs.bib"),
)
#show figure.caption: set align(left)
#show figure.caption: set text(size: 8pt)

= Introduction

The gun control debate in the United States (U.S.) has deep historical roots, going back to the initial draft of the Bill of Rights and the addition of the Second Amendment. This amendment, which grants private citizens the right to keep and bear arms, has been debated for centuries, with two main sides: pro-gun rights and pro-gun control. The gun control debate, as used in this study, will refer to any act of 

#figure(
  image("assets/gun_debate.png", width: 90%),
  caption: [
    Perception of Gun Violence as a Major Problem Increases Among Americans: A Partisan Divide from 2018 to 2023 @schaeffer_key_2023
  ],
) <importance_of_debate>

regulation towards the manufacture, sale, transfer, possession, modification, or use of firearms @kleck1986policy. The U.S. has become more focused on the gun control debate as a growing percentage of U.S. citizens believe that gun violence is a major problem, as shown in @importance_of_debate, and $61%$ of citizens believe it is too effortless to legally obtain a gun in the U.S. @schaeffer_key_2023. This debate mainly focuses on the impact of more guns on society and violence. In this paper, the pro-gun rights side will be referred to simply as pro-rights, and pro-gun control as pro-control.

The members of the pro-rights side believe in their Right to Bear Arms stemming from the 2nd Amendment @spitzer2020politics. They focus on the end of the amendment involving the right of individuals to keep and bear arms. They argue that guns are simply tools and are only as dangerous as the person behind them. Instead, guns can be used to save potential crime victims, and therefore, arming society with more guns will lower violence rates. People most likely to commit murder or other gun-related crimes are shown to have a history of criminal careers and records @kennedy1998homicide, meaning increasing access to guns will not lead to a sudden increase in armed crimes. Additionally, the major points of the pro-rights side come from studies conducted by the Centers for Disease Control and Prevention and the National Academy of Sciences which proved that there was no evidence of any restrictions on gun ownership that had a decrease in gun-related incidents @hahn2003first, @national2005firearms, @boylan2013debate.

The members on the pro-control side mainly use gun violence statistics as their argument, arguing that since guns were designed to kill people or cause harm, more guns will lead to more deaths and incidents @spitzer2020politics. While guns can be used for self-defense, the data shows that guns are more used for murders than for protection against crimes. For self-defense, advocates debate that lower or middle-damage weapons can also be used for protection and lead to less severe injuries and higher recovery rates in hospitals. Some examples of these weapons are bottles, sticks, books, chairs, and bats. However, using a gun leads to such a high jump in the severity of injuries, and the amounts of death and permanent injuries in hospitals skyrocket. They also argue that the Second Amendment is often misinterpreted by focusing on the preamble of the amendment @chemerinsky2004putting, which is about maintaining peace through state militias to protect themselves against federal tyranny. However, only $12.8%$ of pro-control advocates believe in a total ban on guns; instead, the majority of $65.7%$ favor more licensing or stringent measures on gun ownership, such as through background checks or mandatory training @smith2002public.

== Importance of Sentiment Analysis

One form of analysis that has not yet been majorly conducted for the gun control debate is sentiment analysis. Sentiment analysis is a subtype of Natural Language Processing (NLP), which uses algorithms on text or voice input for specific tasks. NLP has many uses, including long short-term memory, word recognition, acoustic modeling, and sentiment analysis @jain2018nlp. Sentiment analysis trains a model to understand text in ways similar to humans to determine the emotional meaning behind the text. As described in @cambria2017practical and @mercer2010emotional, emotions play an important role in rational learning and can influence opinions and attitudes, leading to the strengthening or rejection of beliefs or facts. Using sentiment analysis on the gun control debate could provide unique emotional insights into this issue, such as how people communicate about the advocacy or opposition of gun control, and what emotional appeals are used to sway others.

== Literature Review

The use of NLP, and more specifically sentiment analysis, is not a new field of artificial intelligence but has made some recent strides in the legal field that can outmatch humans in specific tasks, as explained in @frankenreiter2022natural. Using sentiment analysis to understand the gun control debate is another relatively unexplored field, but there are a few papers with useful contributions toward a new understanding of the debate. 

To understand the public sentiment in social media, @wang2016machine served as the most advantageous, as it conducted a sentiment analysis of Twitter posts surrounding the Sandy Hook school shootings in Connecticut, as major public events have been proven to bring out opinionated responses on social media. It was a major tragedy that was “unprecedented in its scale” @wang2016machine[p.~303] and led social media users to focus on and argue about the gun control debate. Using over 700k tweets and acceptable sentiment analysis practices as defined by the field, there was a peak of anti-gun or pro-control sentiment the day of the shootings, but this quickly fell back down to pre-event levels. On the other hand, there was increased pro-rights sentiment which continued for an extended period, showing that many people see public gun ownership as part of the solution, not the problem.

Another study @peterson2019guns utilizes sentiment analysis on court cases in the D.C. Court of Appeals to understand judicial sentiment and opinions toward gun-related incidents, as well as potential external events that had some impact on sentiment. The D.C. Court of Appeals was chosen as D.C. is the second most liberal city in the U.S., with very strict gun regulations and a growing homicide rate. Using the pre-built VADER lexicon-based model, the analysis showed that there was a high level of neutral sentiment with very little fluctuation over time. Both of these factors combined seem “almost inhuman” @peterson2019guns but could be due to the effect of standardization of tone in legal writing. There is also a slight increase in negative sentiment as time goes on, meaning the D.C. Court of Appeals is turning more opposed to gun violence.

There have been other NLP-related papers focused on the gun control debate, such as @liu2019detecting, which showed that the news mainly focuses on the polarized political parties instead of facts of the debate, but this study focuses on sentiment analysis to offer another perspective on this contentious issue.

== Gap in the Field
While the sentiment analysis on Twitter and the D.C. Court of Appeals provided new insights into the gun control debate, there is one source of text that hasn't been looked at by any NLP papers: congressional legislation. With the gun debate being a polarizing issue in the U.S. focused on by the majority of U.S. citizens, it leads to pressure on Congress to represent the public and address the debate through legislative actions following their party views. @bruce1998changing explains the start of Congress' involvement in this debate from the first “assault weapons ban” in 1994, and the increased focus on this issue by both the House of Representatives and the Senate. Legislation is important to look at as it directly affects the gun control debate by strengthening or lessening gun regulations. By conducting sentiment analysis, I would gain a deeper understanding of what language legislators use in legislation, as well as subtle biases that might influence legislative outcomes and public opinion. This can then be used by legislators or legal analysts to draft legislation with more favorable responses to get their policies through.

== Approaches to Sentiment Analysis
There are two main approaches to sentiment analysis: the lexicon-based approach and the machine-learning-based approach. As discussed in @aung2017sentiment, the lexicon-based approach, also known as the rule-based approach, is a traditional method that determines sentiment by relying on predefined rules and lexicons/dictionaries, which are filled with lists of terms and associated sentiment values. The sentiment values measure the strength of the feeling, with positive scores representing positive statements and negative scores representing negative statements. These values are then aggregated to derive the overall sentiment of the text. The lexicons can either be hand-crafted using machine learning techniques to compile a list of words and then manually assign sentiment scores or can alternatively be derived from existing general English lexicons that are downloaded and then edited to tailor the sentiment scores for specific applications, such as fitting the legislative context. Although this method is straightforward, its major drawback is its inability to detect nuances such as sarcasm due to the method getting scores by word and not accounting for the context. However, this limitation is generally not critical in the context of legislative texts, which are expected to be straightforward and unambiguous @baude_statutory, @blinova2023plain, @brannon2018statutory.

The second approach is the machine learning (ML)-based approach. According to @langley1986human, ML is a subset of artificial intelligence that utilizes algorithms and statistical techniques that allow computers to execute specific tasks efficiently without direct instructions, instead depending on pattern recognition and inferential logic. ML involves the creation of mathematical models based on sample data, otherwise referred to as training data, which facilitate predictions or decisions without the need for direct programming of the tasks. Concerning sentiment analysis, there are three main ML algorithms used: deep learning, supervised learning, and unsupervised learning @lecun2015deep. These models use various algorithms to create functions that can predict output values. However, a crucial shortcoming is that it requires extensive amounts of labeled data to get accurate results, as the main purpose is to give machines the ability to analyze and learn like humans. Even with small amounts of training data, ML-based models can start overfitting, meaning they learn to perform very well on the data they have been trained on but fail to accurately predict new, unseen data @hawkins2004problem. This happens because the model ends up capturing noise and anomalies in the training data as if they were significant patterns, leading to a model that is too tailored to the specifics of its training set rather than to underlying general principles.

== Project Goal
Using sentiment analysis on congressional gun legislation. there are two main goals I want to address. First, I want to understand to what extent the sentiments expressed in pro-control versus pro-rights congressional gun legislation differ. This would help me understand the language used by both sides when addressing the gun debate. Next, a more general project goal is to understand what the sentiments expressed in congressional gun legislation reveal about the underlying aspects of the debate. This will involve looking at other factors using the results from the sentiment analysis to influence my new understanding.

= Methodology

== Data Collection

As this study focuses on congressional legislation, all legislative data was sourced from Congress.gov @congressgov, the official website for legislative information. Using the advanced search function with specific search terms and filters, I could identify the legislation relevant to the gun control debate. The search term “gun” and the legislation category option were used to capture a comprehensive list of relevant legislation. To further refine my search, I specified a timeframe spanning congressional sessions 101-117 (January 3rd, 1989, to January 3rd, 2023). The starting point of 1989 was chosen because the legislation text before this session is unavailable, while the ending point in 2023 reflects the ongoing nature of the 118th congressional session, which has not yet concluded and continues to generate new legislative content. Lastly, the status of legislation was set to all options, including failed legislation, as although unsuccessful legislation lacks real-world impact, its value lies in understanding legislators' framing of issues. Even failed legislation can be beneficial as it provides additional insights from legislators.

After conducting the search, I then used the bulk download feature to extract relevant data on each piece of legislation. Using this option, I received the Legislation Number, URL, Congressional Session, Title, Party of Sponsors, and Date of Introduction of each legislation, all of which have a purpose in this study. The Legislation Number serves as a unique identifier, differentiating each piece of legislation and providing information on its origin, including the chamber in which it was introduced and the type of legislation. The Date of Introduction is used later on in the analysis to determine how legislation sentiment changes over time, while the Party of Sponsors will be used to determine the side (pro-control or pro-rights) each political party is on.

While the bulk download feature provides much useful information, it does not provide the actual legislative text to conduct the sentiment analysis. Therefore, I created a web scraper to use the URLs from the bulk download to retrieve the textual data. Using the Selenium Python library @nyamathulla2021review, I simulated a Google Chrome browser to open a modified URL link that points directly to the website with the legislative link and retrieve the HTML source code, which contains all the elements of the website, including the text. I then parsed the HTML code to extract the relevant legislation text using the BeautifulSoup4 Python library @uzun2018comparison. Before saving the text in a CSV file with all of the other information from the bulk download feature on Congress.gov, I created a script to pre-process the legislative text to prepare it for sentiment analysis. The script finds and removes the header and signatories' information before the actual legislation text. Next, the script conducts a series of pre-processing methods as detailed in @jianqiang2017comparison. This involves replacing grammatical contractions with their expanded forms, removing URL links, removing numbers, expanding acronyms to their full form, and removing stop words. Stop words, including “the,” “is,” and “at,” are words that are very common and have no sentiment, which is why they are removed before the sentiment analysis step. The stop words removed from the script were taken from @fox1989stop. With the text pre-processed, it is saved and ready for its classification and sentiment analysis.

== Data Classification

After gathering the legislative data, I classified each piece of legislation as pro-control and pro-rights using the OpenAI Generative Pre-trained Transformer (GPT) Large Language Model (LLM). LLMs are machine learning models with the capability to comprehend and generate human language text after being trained on massive collections of textual data, such as books, articles, and websites @almarie2023editorial. As shown through the experiment @bubeck2023sparks, LLMs have had phenomenal results in fields from vision to coding to math to even societal influences, such as bias, misinformation, human expertise, etc. LLMs can address a wide range of tasks, which sets them apart from earlier models limited to handling particular tasks. GPT uses a wide variety of natural language processing techniques and deep learning methods, as well as the ability to retain information from previous user interactions, to prove personalized humanized responses to each user. I specifically chose the latest OpenAI model, GPT-4, as it was trained on unparalleled amounts of data @achiam2023gpt such that many researchers believe it to be a step towards more general intelligence compared to previous AI models @bubeck2023sparks.

Using the OpenAI API to access GPT-4, I gave it the following prompt: “You will be provided with a legislative text regarding firearms, and your task is to classify it as pro-control or pro-rights. Your response can be one of two things: 'control' or 'rights'.” The prompt explains the input that the model will be receiving, which is a legislative text, as well as its task to categorize it as either pro-control or pro-rights. To further improve the accuracy of the model, the model's temperature hyperparameter was changed to get the most direct output. The sampling temperature, a value from $0.0$ to $2.0$, determines how random a response from the model can be. Higher values make the output more random, while lower values are more focused and concentrated. The study @renze2024effect showed that the GPT-4 model has extremely similar performance values when changing the sampling temperature, but as I wanted the model to return only one word and nothing else as it would impact the script that saves the output, I set the temperature to $0.0$. The other hyperparameters were set to default as they wouldn't have any significant impact on the accuracy in this particular use case. The API was then given the legislation texts to classify and save in CSV files to be used later in the analysis of my results.

== Sentiment Analysis

With the classification of the legislation complete, the next part of the methodology is to conduct the sentiment analysis on the congressional legislation. As mentioned in the Introduction section, there are two commonly used approaches to sentiment analysis, lexicon-based and ML-based. ML-based models perform with the highest accuracy when given training data to draw patterns and conclusions. In this study, training data would consist of sample legislation taken from the overall dataset, with a dictionary of sentiment scores associated with certain words or phrases in the sample legislation. However, this would need to be done by an expert in the field who fully understands the sentiment behind the words, or otherwise, bias could be introduced into the models, leading to inaccurate results. Therefore, lexicon-based models, which require no training data as they only use a pre-defined dictionary of general sentiment in the English language, were the only viable option to generate error-free results.

This study uses the Valence Aware Dictionary for sEntiment Reasoning (VADER) rule-based model introduced in @hutto2014vader. VADER was constructed as a “generalizable, valence-based, human-curated gold standard sentiment lexicon” @hutto2014vader[p.~219] which requires no training data and can easily be applied to various contexts. It is sensitive to the polarity (positive, negative, or neutral) and the intensity of the sentiments expressed. The model returns a value between $-1$, representing negative statements, and $+1$, representing positive statements. Sentiment values between $-0.05$ and $+0.05$ depict a neutral text. Additionally, VADER has real-time analysis capability without being too computationally expensive, while ML-based models require multi-core computer systems with lots of RAM for sentiment analysis.

As discussed in the Introduction, there has been very limited sentiment analysis work in the legal field, or more specifically congressional legislation. Therefore, finding the best lexicon-based model needs to be centered around the general English language. The models I analyzed were compared across four main domains: social media texts and posts, New York Times opinion editorials, Rotten Tomatoes movie reviews, and technical product reviews. Compared to other frequently employed lexicons specifically Linguistic Inquiry Word Count, General Inquirer, Affective Norms for English Words, SentiWordNet, SenticNet, Word-Sense Disambiguation using WordNet, and Hu-Liu04 opinion, VADER achieves first in the social media category and second in the others. Against lexicons created by ML-oriented techniques, such as Naive Bayes, Maximum Entropy, and Support Vector Machine algorithms, VADER outperformed in three out of the four domains tested compared to the ML algorithms that are specifically trained in that domain, highlighting the high level of generalizability of the VADER model without any context-specific optimization.

To evaluate VADER's performance relative to human raters, the developers conducted a correlation study assessing both sentiment polarity and intensity. In measuring intensity, VADER achieved an accuracy of $88.1%$ closely approaching the human raters' accuracy of $88.8%$. However, VADER surpassed human raters in identifying sentiment polarity, demonstrating a higher accuracy rate of $96%$ compared to the humans' $84%$.

Therefore, VADER was the model chosen to conduct the sentiment analysis on legislative texts. Since the data collection step of the methodology already pre-processed the text and prepared it for sentiment analysis, the texts are now run in the VADER model and have overall sentiment scores saved in a CSV file.

== Statistics

To understand how the legislation sentiments differ, a statistics test must be implemented to compare the sentiments of pro-control and pro-rights legislation together. A T-test was chosen as it would help determine if there is a significant difference between the means of the two sides and how they are related. Specifically, I will be conducting a 2 independent sample T-test, as I go in with the assumption that the sentiments of the sides are independent and have no effect on each other.

However, the specific version of the test will depend on whether the distribution of both samples follows the normal, or Gaussian, distribution. What this means is that the majority of sentiment scores will be close to the mean (average) score, and the overall distribution of sentiment scores will be symmetric around the mean. Additionally, the extreme values (both highly positive and highly negative sentiments) are less common and represent outlier sentiments that are not typical of the overall set of legislation. To calculate whether a distribution is normal, a Shapiro-Wilk test can be conducted to prove if the samples fit a normal distribution. With a Shapiro-Wilk test, the null hypothesis, which I can either reject or fail to reject, is that the set of data comes from a normal distribution. If the null hypothesis is rejected, the alternative hypothesis, which is that the set of data does not come from a normal distribution, is accepted.

If both samples follow a normal distribution, an Independent Samples T-test will be conducted. If not, then a Mann-Whitney U test will be conducted. With both tests, the null hypothesis is always that there is no significant difference between both samples. Therefore, the null hypothesis would be that there is no difference (in terms of central tendency) in sentiments between pro-control and pro-rights congressional gun legislation. The alternative hypothesis is that there is a difference (in terms of central tendency) in sentiments between pro-control and pro-rights congressional gun legislation.

In statistical hypothesis testing, the alpha value, also known as the significance level, is a threshold used to determine whether to reject the null hypothesis. The commonly used alpha value by researchers is set to $0.05$ and represents a $5%$ risk of concluding that a difference exists when there is none, which is considered an acceptable level of risk in many research studies. If the p-value from the statistical tests is lower than the alpha value, the null hypothesis is rejected, meaning the observed effect is statistically significant and likely not due to random chance. If the p-value is greater than the alpha value, I fail to reject the null hypothesis, meaning there is insufficient evidence to support the existence of a significant effect.

= Results & Analysis

From the data collection step of the methodology, $2030$ unique pieces of legislation were taken from the Congress.gov site and pre-processed. Of that, $62.81%$ $(1275)$ of legislative measures were primarily sponsored by the Democratic political party, and $37.19%$ $(755)$ were sponsored by the Republican party. There was only one legislation sponsored by the Independent party but it was added to the Democratic categorization as Congress.gov labeled it as “Independent Democratic”. Next, the legislative texts were classified as either pro-control or pro-rights. $69.26%$ $(1406)$ of the legislation was classified on the pro-control side, while $30.74%$ $(624)$ was on the pro-rights side.

To understand which side of the gun control debate each political party takes in congressional legislation, a bar chart, as shown in @graph_bar_parties, was created to compare the policy focus of the political parties. It shows that the Democratic party has $~5.85$ times more pro-control legislation than pro-rights, meaning the Democratic party leans heavily towards the pro-control side of the gun control debate. Meanwhile, the Republican party has only $~1.38$ times more pro-rights than pro-control legislation, proving that the Republican party is more split on this issue but does still lean more towards the pro-rights side.

#figure(
  image("assets/graph_bar_parties.svg", width: 100%),
  caption: [
    Comparative Bar Chart of Legislative Trends: This graph presents a quantitative analysis of pro-control and pro-rights legislation as supported by the Democratic and Republican parties, revealing the contrasting priorities and policy focus within American political dynamics.
  ],
) <graph_bar_parties>

@graph_scatterplot is a scatterplot of sentiment vs time to see how the legislation sentiment changed over time. It shows that the sentiments of the legislation are very polarized, with a lot of legislative texts having extreme values near $1.0$ and $-1.0$. The equation of the general trend line of the data, which represents how time affects the sentiment, is:\ $y = -4.18*10^{-6}x + 0.287$, where $y$ is the sentiment score and $x$ is the legislation's date of introduction. The equation shows that as time goes on, the sentiment in legislation does become more negative towards the neutral line but in extremely small increments. For the trend line to reach the full neutrality score of $0.0$, it would take over 68k years from the start date, January 3rd, 1989.

#figure(
  image("assets/graph_scatterplot.svg", width: 100%),
  caption: [
    Temporal Analysis of Sentiment in Congressional Gun Legislation: This scatter plot with a trend line demonstrates the fluctuation in sentiment scores of gun legislation over three decades, as evaluated by the VADER sentiment analysis, indicating the evolving emotional tone in legislative language.
  ],
) <graph_scatterplot>

The following histogram, @graph_histogram_overall, represents the overall distribution of legislation based on sentiments to visualize the polarity of the overall legislation, which was also seen in @graph_scatterplot. It visualizes that congressional gun legislation is extremely polarized. There is barely any legislation in the neutral area, which is a sentiment value between $-0.05$ and $0.05$. Instead, the histogram has a bimodal shape, with the two peaks being at the most positive and most negative scores. Additionally, the variance of the histogram, which tells us how spread out a set of data is, is $0.8617140$, meaning the data is very spread out.

#figure(
  image("assets/graph_histogram_overall.svg", width: 100%),
  caption: [
    Histogram of Congressional Gun Legislation Sentiment: This histogram provides a distribution of sentiment scores across all gun-related legislation, as measured by VADER sentiment analysis, to reflect the general emotional tone within legislative language.
  ],
) <graph_histogram_overall>

While @graph_histogram_overall shows the overall legislation polarity, the main purpose of the study is to understand how the sentiment differs in legislation from the two sides of the gun control debate. This was done by creating histograms for legislation from each side to see how they compare against each other as shown in @graph_histogram_control and @graph_histogram_rights.

#figure(
  image("assets/graph_histogram_control.svg", width: 100%),
  caption: [
    Histogram of Pro-Control Congressional Gun Legislation Sentiment: This histogram provides a distribution of sentiment scores within pro-control gun-related legislation, as measured by VADER sentiment analysis, to reflect the emotional tone in the language of gun control advocacy.
  ],
) <graph_histogram_control>

#figure(
  image("assets/graph_histogram_rights.svg", width: 100%),
  caption: [
    Histogram of Pro-Rights Congressional Gun Legislation Sentiment: This histogram provides a distribution of sentiment scores within pro-rights gun-related legislation, as measured by VADER sentiment analysis, to reflect the emotional tone in the language of gun rights advocacy.
  ],
) <graph_histogram_rights>

Both of these histograms are bimodal, with their peaks being at the two ends of the sentiment scores. The variances for the pro-control and pro-rights data are $0.8762$ and $0.7305$, respectively. This difference in variance between the two histograms is significant, indicating that while both pro-control and pro-rights legislative texts are polarized, the pro-control texts exhibit a slightly higher degree of spread in sentiment values. This higher variance suggests that the language used in pro-control legislation is more diverse in its emotional expression compared to pro-rights texts. Essentially, pro-control advocates use a wider range of sentiment, potentially reflecting a more varied approach in how they present their arguments or address the issue. Conversely, the lower variance in the pro-rights texts may imply a more consistent use of language. This could indicate a more unified stance or a narrower range of emotional appeal in the rhetoric of gun rights advocacy. The bimodal distributions in both histograms affirm that both sides of the debate strongly emphasize positive and negative sentiments, with little room for neutral language. This polarization could be a reflection of the intense and often contentious nature of the gun control debate in the U.S.

A Shapiro-Wilk test on both sets of data, pro-control & pro-rights legislation sentiment, returns a p-value of $<0.001$ for both samples. As the p-values are lower than the alpha value of $0.05$, the null hypothesis, which was that the sets of data fit a normal distribution, is rejected. Since the samples are not normal, a Mann-Whitney U test is used to determine a significant difference and relation between both groups. The test also results in a p-value of $<0.001$, meaning the null hypothesis is rejected. Therefore, the statistics show that there is a difference (in terms of central tendency) in sentiments between the pro-control and pro-rights congressional gun legislation. This can also be seen in @graph_histogram_control & @graph_histogram_rights, as the pro-control histogram has more legislation with an overall negative sentiment score, but the pro-rights histogram has the opposite with more legislation with positive sentiment scores.

= Discussion & Conclusion

== Implications

This project gave additional insights into the gun control debate using a sentiment analysis approach with the VADER lexicon-based model. 

It found that there is a significant difference between the emotions in legislation from either side. The histograms of pro-control and pro-rights legislation in @graph_histogram_control and @graph_histogram_rights show that the pro-control legislation has more negative legislation sentiment, while pro-rights legislation has more positive legislation sentiment. The analysis of the variances in sentiment provided insights into the rhetorical strategies employed by both sides. It shows that pro-control legislation has a broader range of emotional expression compared to the more consistent pro-rights rhetoric, potentially affecting public perception and policy discussions.

The study also revealed a stark polarization in legislative sentiment on gun control, closely aligning with partisan divisions—Democrats favoring pro-control and Republicans displaying a slight preference for pro-rights.

Lastly, the scatterplot in @graph_scatterplot shows us that, over time, legislation becomes more negative in sentiment but in infinitesimally small values, requiring decades for it to reach full neutrality.

Following further analysis, this study can be implemented by legislators to foster more bipartisan support for legislation by focusing on common ground that appeals to both the positive and negative sentiment areas. Legal analysts could also use this research as part of a predictive framework to anticipate the likelihood of legislation passing, as this could influence legal advisories, compliance activities, and risk assessments for businesses and NGOs. Lastly, understanding sentiment could assist in drafting legislation to receive a certain response from the rest of Congress and the public.

== Limitations

Although this study provides significant contributions to the field, it does have potential limitations that affect the understanding of the analysis. One of the primary constraints comes from the VADER model. Its lexicon, while extensive and known for its generalizability & high performance in many fields, does have inaccuracies in sentiment analysis. Since the VADER lexicon was created for the general English language and not specifically for the congressional legislative context, there may be words or phrases that have different sentiments between general and legislative-specific language. This would affect the sentiment scores used in the analysis, but due to the sheer length of legislation and the individual word sentiment scores being aggregated for one overall score, this limitation shouldn't majorly impact the sentiment score for individual legislation. Additionally, another constraint comes from the search term used during the data collection phase of the methodology. The term “gun” may not have captured all relevant legislation, potentially omitting relevant texts that use different terminology such as “firearm” or “assault rifles”. However, there is a very small chance that legislation related to the gun control debate would not include the term “gun”, meaning this constraint would have a very minimal impact on this study. Another limitation is the inclusion of duplicate legislation in the dataset, which could skew the results by adding multiple of the same sentiment scores to the results. Duplicate legislation can come from legislation entering both chambers of Congress, thus appearing duplicated. However, amendments and revisions from each house often modify the text slightly, affecting sentiment analysis, and therefore, these duplicates cannot just be removed. Nevertheless, a comparison of the titles of the legislation shows that only $9%$ of legislation is duplicated, which isn't much and shouldn't have a major impact on the overall analysis.


== Future Directions

Despite the findings and implications of this study, there is still much more to explore regarding the gun control debate. As shown in the analysis of legislation sentiment vs time in @graph_scatterplot, the trend line shows that legislation is becoming increasingly negative, yet in slight increments. However, there is no way to confirm whether or not the legislation sentiment is becoming neutral and will stick near the x-axis, or will continue past it and increasingly become more negative. Therefore, this study should be continuously replicated in the future after the conclusion of each congressional session to gain a better sense of how sentiment changes over time.

Additionally, future research could work to create an extensive training dataset to create various ML-based models for cross-validation of the results of this study. Using the help of an expert in congressional legislation, the training dataset could include a small percentage of legislation from each congressional session and then be labeled with sentiment scores (both polarity and intensity). With the training dataset, advanced ML-based models, such as those mentioned in the Introduction, can be implemented for sentiment analysis and then compared to the VADER sentiment scores to find any major differences that could lead to a new understanding.

To learn more about the gun control debate, a thematic analysis could be implemented on congressional gun legislation. A thematic analysis would explain the specific themes or points brought up in the legislation, and can then be used in Congress to find common ground for bipartisan support to improve the gun violence situation.

Lastly, the analysis does not account for the influence of media coverage, public opinion, or external political events that could also affect legislative sentiment. These factors should be considered when interpreting the findings and could be addressed in future research to enhance the understanding of sentiment trends in congressional gun legislation.
